{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811e125c",
   "metadata": {},
   "source": [
    "This project completely uses Agno AI Agent. Although it has many features but it was often a struggle, it felt that using embedding independently would have been much easier. Also it's difficult to locate the updates attributes name everytime. Langchain has an upper hand over this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79a808a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/lancedb/__init__.py:220: UserWarning: lance is not fork-safe. If you are using multiprocessing, use spawn instead.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygithub in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: lancedb in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.22.0)\n",
      "Requirement already satisfied: sentence-transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: agno in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.4.4)\n",
      "Requirement already satisfied: pynacl>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pygithub) (1.5.0)\n",
      "Requirement already satisfied: requests>=2.14.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pygithub) (2.32.3)\n",
      "Requirement already satisfied: pyjwt>=2.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyjwt[crypto]>=2.4.0->pygithub) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pygithub) (4.13.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pygithub) (2.2.3)\n",
      "Requirement already satisfied: Deprecated in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pygithub) (1.2.18)\n",
      "Requirement already satisfied: deprecation in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lancedb) (2.1.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lancedb) (2.1.0)\n",
      "Requirement already satisfied: overrides>=0.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lancedb) (7.7.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lancedb) (24.1)\n",
      "Requirement already satisfied: pyarrow>=14 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lancedb) (20.0.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lancedb) (2.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from lancedb) (4.67.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: docstring-parser in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agno) (0.16)\n",
      "Requirement already satisfied: gitpython in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agno) (3.1.44)\n",
      "Requirement already satisfied: httpx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agno) (0.28.1)\n",
      "Requirement already satisfied: pydantic-settings in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agno) (2.9.1)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agno) (1.1.0)\n",
      "Requirement already satisfied: python-multipart in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agno) (0.0.20)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agno) (14.0.0)\n",
      "Requirement already satisfied: tomli in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agno) (2.2.1)\n",
      "Requirement already satisfied: typer in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from agno) (0.15.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=1.10->lancedb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=1.10->lancedb) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic>=1.10->lancedb) (0.4.0)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pyjwt[crypto]>=2.4.0->pygithub) (44.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.4.0->pygithub) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.4.0->pygithub) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.14.0->pygithub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.14.0->pygithub) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.14.0->pygithub) (2025.4.26)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.6.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from Deprecated->pygithub) (1.17.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gitpython->agno) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython->agno) (5.0.2)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx->agno) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx->agno) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx->agno) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from anyio->httpx->agno) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->agno) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->agno) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->agno) (0.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typer->agno) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from typer->agno) (1.5.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pygithub lancedb sentence-transformers agno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cf7a1761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from github import Github\n",
    "from itertools import islice    #con of popular repos, they have many issues !!, so we trim to test 20 issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a760a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "g=Github(\"your_github_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67c1fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = g.get_repo(\"NVIDIA/physicsnemo\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ba3bf9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_issues = [\n",
    "    {\n",
    "        \"number\": issue.number,\n",
    "        \"title\": issue.title,\n",
    "        \"body\": issue.body or \"\",\n",
    "        \"url\": issue.html_url\n",
    "    }\n",
    "    for issue in islice(repo.get_issues(state=\"all\"), 20)\n",
    "    if issue.pull_request is None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "00b3c972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop to get all issues\n",
    "texts = [f\"{issue['title']} {issue['body']}\" for issue in raw_issues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0afd345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding\n",
    "from agno.embedder.sentence_transformer import SentenceTransformerEmbedder\n",
    "\n",
    "embedder = SentenceTransformerEmbedder(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8ba8455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.vectordb.lancedb import LanceDb\n",
    "from agno.agent import AgentKnowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e916c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lanceDB baby directly store with embedding model\n",
    "vector_db = LanceDb(table_name=\"github-issues\",embedder=embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "86053aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_db.search(query=\"login issue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ee9b7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join([f\"Issue {i+1}:\\n{doc.content}\" for i, doc in enumerate(results)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "40658eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a helpful assistant. Here are some GitHub issues:\n",
    "\n",
    "{context}\n",
    "\n",
    "Summarize the common problems or patterns described in these issues.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"your_api_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3d0e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.agent import Agent\n",
    "from agno.models.google import Gemini  \n",
    "from agno.tools.reasoning import ReasoningTools\n",
    "\n",
    "agent = Agent(\n",
    "    model=Gemini(id=\"models/gemini-2.0-flash-lite\"),\n",
    "    tools=[ReasoningTools(add_instructions=True)],\n",
    "    instructions=[\n",
    "        \"Identify important themes\",\n",
    "        \"Group similar issues\",\n",
    "        \"Highlight core problems in one line\"\n",
    "    ],\n",
    "    markdown=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9c94cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "08631848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- Gemini Summary ----\n",
      "\n",
      "RunResponse(content=\"Here's a breakdown of the common problems and patterns across the provided GitHub issues:\\n\\n*   **Issue 1 & 2: Regression and Diffusion Model Errors after PR merge:** Both issues describe errors encountered in the CorrDiff example after a specific pull request. Issue 1 is a `TypeError` related to an unexpected keyword argument in `RegressionLoss`. Issue 2 is a `RuntimeError` related to a type mismatch during generation, potentially linked to changes in the PR. **Core Problem:** Errors introduced in the CorrDiff example after the specified PR, specifically impacting training and generation, potentially due to inconsistencies in the code or data types.\\n\\n*   **Issue 3: NaN values with `@StaticCaptureEvaluateNoGrad`:** This issue highlights a problem where using a specific decorator (`@StaticCaptureEvaluateNoGrad`) leads to the generation of NaN values during inference, while standard PyTorch inference does not. **Core Problem:** NaN values appear during inference when using a specific decorator, indicating a potential issue with the decorator's implementation or interaction with the model.\\n\\n*   **Issue 4: HRRR Dataset Usage and Computational Cost for StormCast:** This is a feature request focused on the StormCast model. The user is asking about the best practices for using HRRR datasets at different levels (natural/hybrid vs. pressure) and wants an estimate of the training's computational cost. **Core Problem:** Seeking guidance on optimal HRRR data usage and computational cost for training the StormCast model.\\n\\n*   **Issue 5: CorrDiff Validation and Early Stopping Documentation:** This issue is a documentation request concerning the validation process and early stopping mechanisms in the CorrDiff example's training script. The user is confused about how validation data is handled and whether early stopping is supported. **Core Problem:** Lack of clarity and potential missing documentation regarding the validation process and early stopping in the CorrDiff training script.\\n\\nIn essence, the issues cover these themes:\\n- Code bugs after a PR merge\\n- Problems with inference with the @StaticCaptureEvaluateNoGrad decorator\\n- Feature request around best practice on dataset and training costs\\n- Doc request around validation and early stopping in the training script.\\n\", content_type='str', thinking=None, reasoning_content=None, event='RunResponse', messages=[Message(role='system', content='<instructions>\\n- Identify important themes\\n- Group similar issues\\n- Highlight core problems in one line\\n</instructions>\\n\\n<additional_information>\\n- Use markdown to format your answers.\\n</additional_information>\\n\\n<reasoning_instructions>\\nYou have access to the `think` and `analyze` tools to work through problems step-by-step and structure your thought process. You must ALWAYS `think` before making tool calls or generating a response.\\n\\n1. **Think** (scratchpad):\\n    - Purpose: Use the `think` tool as a scratchpad to break down complex problems, outline steps, and decide on immediate actions within your reasoning flow. Use this to structure your internal monologue.\\n    - Usage: Call `think` before making tool calls or generating a response. Explain your reasoning and specify the intended action (e.g., \"make a tool call\", \"perform calculation\", \"ask clarifying question\").\\n\\n2. **Analyze** (evaluation):\\n    - Purpose: Evaluate the result of a think step or a set of tool calls. Assess if the result is expected, sufficient, or requires further investigation.\\n    - Usage: Call `analyze` after a set of tool calls. Determine the `next_action` based on your analysis: `continue` (more reasoning needed), `validate` (seek external confirmation/validation if possible), or `final_answer` (ready to conclude).\\n    - Explain your reasoning highlighting whether the result is correct/sufficient.\\n\\n## IMPORTANT GUIDELINES\\n- **Always Think First:** You MUST use the `think` tool before making tool calls or generating a response.\\n- **Iterate to Solve:** Use the `think` and `analyze` tools iteratively to build a clear reasoning path. The typical flow is `Think` -> [`Tool Calls` if needed] -> [`Analyze` if needed] -> ... -> `final_answer`. Repeat this cycle until you reach a satisfactory conclusion.\\n- **Make multiple tool calls in parallel:** After a `think` step, you can make multiple tool calls in parallel.\\n- **Keep Thoughts Internal:** The reasoning steps (thoughts and analyses) are for your internal process only. Do not share them directly with the user.\\n- **Conclude Clearly:** When your analysis determines the `next_action` is `final_answer`, provide a concise and accurate final answer to the user.\\n</reasoning_instructions>', name=None, tool_call_id=None, tool_calls=None, audio=None, images=None, videos=None, files=None, audio_output=None, image_output=None, thinking=None, redacted_thinking=None, provider_data=None, citations=None, reasoning_content=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, add_to_agent_memory=True, from_history=False, metrics=MessageMetrics(input_tokens=0, output_tokens=0, total_tokens=0, audio_tokens=0, input_audio_tokens=0, output_audio_tokens=0, cached_tokens=0, reasoning_tokens=0, prompt_tokens=0, completion_tokens=0, prompt_tokens_details=None, completion_tokens_details=None, additional_metrics=None, time=None, time_to_first_token=None, timer=None), references=None, created_at=1746625028), Message(role='user', content='\\nYou are a helpful assistant. Here are some GitHub issues:\\n\\nIssue 1:\\nüêõ[BUG]: TypeError: RegressionLoss.__call__() got an unexpected keyword argument \\'use_patch_grad_acc\\' ### Version\\n\\nLatest from main branch\\n\\n### On which installation method(s) does this occur?\\n\\nSource\\n\\n### Describe the issue\\n\\nFollowing this [PR](https://github.com/NVIDIA/physicsnemo/pull/809), the CorrDiff example has an error in the regression training:\\n\\n```python\\n[2025-05-02 10:42:19,754][main][INFO] - Using dataset: hrrr_mini\\n[2025-05-02 10:42:19,755][main][INFO] - Saving the outputs in /mnt/azureml/cr/j/.../exe/wd\\n[2025-05-02 10:42:36,214][main][INFO] - Patch-based training disabled\\n[2025-05-02 10:42:36,512][main][INFO] - Using 4 gradient accumulation rounds\\n[2025-05-02 10:42:36,531][checkpoint][WARNING] - Provided checkpoint directory /mnt/azureml/cr/j/.../cap/data-capability/wd/checkpoint_dir/checkpoints_regression does not exist, skipping load\\n[2025-05-02 10:42:36,531][main][INFO] - Training for 2000000 images...\\nError executing job with overrides: [\\'++dataset.data_path=...\\', \\'++dataset.stats_path=...\\', \\'++training.hp.total_batch_size=256\\', \\'++training.hp.batch_size_per_gpu=64\\', \\'++training.perf.dataloader_workers=1\\', \\'++training.io.checkpoint_dir=...\\']\\nTraceback (most recent call last):\\n  File \"/mnt/azureml/cr/j/.../exe/wd/train.py\", line 728, in <module>\\n    main()\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/main.py\", line 94, in decorated_main\\n    _run_hydra(\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\\n    _run_app(\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 457, in _run_app\\n    run_and_report(\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 223, in run_and_report\\n    raise ex\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 220, in run_and_report\\n    return func()\\n           ^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 458, in <lambda>\\n    lambda: hydra.run(\\n            ^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/hydra.py\", line 132, in run\\n    _ = ret.return_value\\n        ^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/core/utils.py\", line 260, in return_value\\n    raise self._return_value\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/core/utils.py\", line 186, in run_job\\n    ret.return_value = task_function(task_cfg)\\n                       ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/mnt/azureml/cr/j/.../exe/wd/train.py\", line 493, in main\\n    loss = loss_fn(**loss_fn_kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\\nTypeError: RegressionLoss.__call__() got an unexpected keyword argument \\'use_patch_grad_acc\\'\\n```\\n\\nIn the PR, I see that the `use_patch_grad_acc` keyword argument was [added](https://github.com/NVIDIA/physicsnemo/pull/809/files#diff-9ed4af76e30456232d3d981f81f3b13b006633a9031f5639462e5b93f4a1ff2d) to `ResidualLoss` but not `RegressionLoss`.\\n\\nShould the same change be applied there?\\n\\nIssue 2:\\nüêõ[BUG]: RuntimeError: Input type (c10::Half) and bias type (float) should be the same ### Version\\n\\nLatest from main branch\\n\\n### On which installation method(s) does this occur?\\n\\nSource\\n\\n### Describe the issue\\n\\nFollowing this [PR](https://github.com/NVIDIA/physicsnemo/pull/809), the CorrDiff example has an error in the generation (see traceback below).\\n\\nThe weights for both regression and diffusion are new following this PR too.\\n\\n```python\\n[2025-05-06 17:09:31,605][generate][INFO] - Using dataset: hrrr_mini\\n[2025-05-06 17:09:48,205][generate][INFO] - Patch-based training disabled\\n[2025-05-06 17:09:48,205][generate][INFO] - Loading residual network from \"/mnt/azureml/cr/j/.../cap/data-capability/wd/INPUT_diffusion_checkpoint_path/EDMPrecondSuperResolution.0.8000000.mdlus\"...\\n[2025-05-06 17:09:49,114][generate][INFO] - Loading network from \"/mnt/azureml/cr/j/.../cap/data-capability/wd/INPUT_regression_checkpoint_path/UNet.0.2000128.mdlus\"...\\n[2025-05-06 17:09:49,426][generate][INFO] - Generating images, saving results to /mnt/azureml/cr/j/.../cap/data-capability/wd/output_filename/sample.nc...\\n[2025-05-06 17:09:50,195][generate][INFO] - starting index: 0\\n/usr/local/lib/python3.12/dist-packages/physicsnemo/models/diffusion/layers.py:701: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast(\\'cuda\\', args...)` instead.\\n  with amp.autocast(enabled=self.amp_mode):\\nError executing job with overrides: [\\'++dataset.data_path=/mnt/azureml/cr/j/.../cap/data-capability/wd/INPUT_data_path/hrrr_mini_train.nc\\', \\'++dataset.stats_path=/mnt/azureml/cr/j/.../cap/data-capability/wd/INPUT_stats_path/stats.json\\', \\'++generation.io.reg_ckpt_filename=/mnt/azureml/cr/j/.../cap/data-capability/wd/INPUT_regression_checkpoint_path/UNet.0.2000128.mdlus\\', \\'++generation.io.res_ckpt_filename=/mnt/azureml/cr/j/.../cap/data-capability/wd/INPUT_diffusion_checkpoint_path/EDMPrecondSuperResolution.0.8000000.mdlus\\', \\'++generation.io.output_filename=/mnt/azureml/cr/j/.../cap/data-capability/wd/output_filename/sample.nc\\']\\nTraceback (most recent call last):\\n  File \"/mnt/azureml/cr/j/.../exe/wd/generate.py\", line 390, in <module>\\n    main()\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/main.py\", line 94, in decorated_main\\n    _run_hydra(\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\\n    _run_app(\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 457, in _run_app\\n    run_and_report(\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 223, in run_and_report\\n    raise ex\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 220, in run_and_report\\n    return func()\\n           ^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/utils.py\", line 458, in <lambda>\\n    lambda: hydra.run(\\n            ^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/_internal/hydra.py\", line 132, in run\\n    _ = ret.return_value\\n        ^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/core/utils.py\", line 260, in return_value\\n    raise self._return_value\\n  File \"/usr/local/lib/python3.12/dist-packages/hydra/core/utils.py\", line 186, in run_job\\n    ret.return_value = task_function(task_cfg)\\n                       ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/mnt/azureml/cr/j/.../exe/wd/generate.py\", line 344, in main\\n    image_out = generate_fn()\\n                ^^^^^^^^^^^^^\\n  File \"/mnt/azureml/cr/j/.../exe/wd/generate.py\", line 192, in generate_fn\\n    image_reg = regression_step(\\n                ^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/physicsnemo/utils/corrdiff/utils.py\", line 84, in regression_step\\n    x = net(x=x_hat[0:1], img_lr=img_lr)\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\\n    return forward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/physicsnemo/models/diffusion/unet.py\", line 165, in forward\\n    F_x = self.model(\\n          ^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\\n    return forward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/physicsnemo/models/diffusion/song_unet.py\", line 703, in forward\\n    return super().forward(x, noise_labels, class_labels, augment_labels)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/physicsnemo/models/diffusion/song_unet.py\", line 450, in forward\\n    x = block(x, emb)\\n        ^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\\n    return forward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/physicsnemo/models/diffusion/layers.py\", line 703, in forward\\n    x = self.proj(attn.reshape(*x.shape)).add_(x)\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\\n    return self._call_impl(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\\n    return forward_call(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \"/usr/local/lib/python3.12/dist-packages/physicsnemo/models/diffusion/layers.py\", line 285, in forward\\n    x = torch.nn.functional.conv2d(x, w, padding=w_pad, bias=b)\\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\nRuntimeError: Input type (c10::Half) and bias type (float) should be the same\\n```\\n\\n### Minimum reproducible example\\n\\n```shell\\nDefault CorrDiff example\\n```\\n\\nIssue 3:\\nüêõ[BUG]: @StaticCaptureEvaluateNoGrad decorator can cause NaN values to show up during inference ### Version\\n\\n24.01\\n\\n### On which installation method(s) does this occur?\\n\\nSource\\n\\n### Describe the issue\\n\\nInferencing my squeezeformer using the @StaticCaptureEvaluateNoGrad decorator can cause NaN values. When inferencing with default pytorch syntax, I do not encounter any such problem.\\n\\n```\\n@StaticCaptureEvaluateNoGrad(model=model, use_graphs=False)\\ndef eval_step_forward(my_model, invar):\\n    return my_model(invar)\\n...\\nIn [19]: output_modulus = eval_step_forward(model, data_input)\\n\\nIn [20]: torch.isnan(output_modulus).any()\\nOut[20]: tensor(True, device=\\'cuda:0\\')\\n\\nIn [21]: with torch.no_grad():\\n    ...:     output_pytorch = model(data_input)\\n    ...: \\n\\nIn [22]: torch.isnan(output_pytorch).any()\\nOut[22]: tensor(False, device=\\'cuda:0\\')\\n```\\n\\n### Minimum reproducible example\\n\\n```shell\\n\\n```\\n\\n### Relevant log output\\n\\n```shell\\n\\n```\\n\\n### Environment details\\n\\n```shell\\nUsing a container on NERSC perlmutter:\\n\\n#SBATCH --image=nvcr.io/nvidia/modulus/modulus:24.01\\n```\\n\\nIssue 4:\\nüöÄ[FEA]: Shall I use HRRR datasets in natural/hybrid model levels when training the StormCast model? ### Is this a new feature, an improvement, or a change to existing functionality?\\n\\nChange\\n\\n### How would you describe the priority of this feature request\\n\\nMedium\\n\\n### Please provide a clear description of problem you would like to solve.\\n\\nDear StormCast Team,\\nI am currently working on training the StormCast model using the HRRR dataset over the western US. Should I train the model using the HRRR data at the natural/hybrid model levels? I noticed that the input state weather variables are provided at these levels (https://catalog.ngc.nvidia.com/orgs/nvidia/teams/modulus/models/stormcast-v1-era5-hrrr#:~:text=Input%20state%20weather,refc). What if I use the data at pressure levels to train the model?\\n\\nAdditionally, could you please provide an estimate of the computational cost associated with training the model?\\n\\nThank you for your time to read and address my questions!\\n\\n### Describe any alternatives you have considered\\n\\n_No response_\\n\\nIssue 5:\\nüìö[DOC]: CorrDiff Validation and Early Stopping ### How would you describe the priority of this documentation request\\n\\nMedium\\n\\n### Is this for new documentation, or an update to existing docs?\\n\\nUpdate\\n\\n### Describe the incorrect/future/missing documentation\\n\\nIn CorrDiffs ```train.py``` there is support for logging validation loss based on the presense of a validation key in the training config yaml. In [dataset.py](https://github.com/NVIDIA/physicsnemo/blob/ad863fdb8ff2e263551e4181b7799f79a4c5177d/examples/generative/corrdiff/datasets/dataset.py#L121) it looks like the training dataset configuration is copied for repurposing for validation with the option to update the configuration. I can see how one would provide a secondary path to a separate dataset used for validation, which would replace the dataset.data_path in either the training config (https://github.com/climacell/physicsnemo/blob/main/examples/generative/corrdiff/conf/config_training_hrrr_mini_regression.yaml) or via hydra override. Is this the intended validation process? I am confused by the```test_train_split``` boolean used to initiate the validation flow if the validation is done on a separate dataset and not actaully split out from the training dataset. \\n\\nAdditionally, if validation logging is supported, is there configurable early stopping for the regression? \\n\\nThanks for clarity. \\n\\nSummarize the common problems or patterns described in these issues.\\n', name=None, tool_call_id=None, tool_calls=None, audio=None, images=None, videos=None, files=None, audio_output=None, image_output=None, thinking=None, redacted_thinking=None, provider_data=None, citations=None, reasoning_content=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, add_to_agent_memory=True, from_history=False, metrics=MessageMetrics(input_tokens=0, output_tokens=0, total_tokens=0, audio_tokens=0, input_audio_tokens=0, output_audio_tokens=0, cached_tokens=0, reasoning_tokens=0, prompt_tokens=0, completion_tokens=0, prompt_tokens_details=None, completion_tokens_details=None, additional_metrics=None, time=None, time_to_first_token=None, timer=None), references=None, created_at=1746625028), Message(role='assistant', content=\"Here's a breakdown of the common problems and patterns across the provided GitHub issues:\\n\\n*   **Issue 1 & 2: Regression and Diffusion Model Errors after PR merge:** Both issues describe errors encountered in the CorrDiff example after a specific pull request. Issue 1 is a `TypeError` related to an unexpected keyword argument in `RegressionLoss`. Issue 2 is a `RuntimeError` related to a type mismatch during generation, potentially linked to changes in the PR. **Core Problem:** Errors introduced in the CorrDiff example after the specified PR, specifically impacting training and generation, potentially due to inconsistencies in the code or data types.\\n\\n*   **Issue 3: NaN values with `@StaticCaptureEvaluateNoGrad`:** This issue highlights a problem where using a specific decorator (`@StaticCaptureEvaluateNoGrad`) leads to the generation of NaN values during inference, while standard PyTorch inference does not. **Core Problem:** NaN values appear during inference when using a specific decorator, indicating a potential issue with the decorator's implementation or interaction with the model.\\n\\n*   **Issue 4: HRRR Dataset Usage and Computational Cost for StormCast:** This is a feature request focused on the StormCast model. The user is asking about the best practices for using HRRR datasets at different levels (natural/hybrid vs. pressure) and wants an estimate of the training's computational cost. **Core Problem:** Seeking guidance on optimal HRRR data usage and computational cost for training the StormCast model.\\n\\n*   **Issue 5: CorrDiff Validation and Early Stopping Documentation:** This issue is a documentation request concerning the validation process and early stopping mechanisms in the CorrDiff example's training script. The user is confused about how validation data is handled and whether early stopping is supported. **Core Problem:** Lack of clarity and potential missing documentation regarding the validation process and early stopping in the CorrDiff training script.\\n\\nIn essence, the issues cover these themes:\\n- Code bugs after a PR merge\\n- Problems with inference with the @StaticCaptureEvaluateNoGrad decorator\\n- Feature request around best practice on dataset and training costs\\n- Doc request around validation and early stopping in the training script.\\n\", name=None, tool_call_id=None, tool_calls=None, audio=None, images=None, videos=None, files=None, audio_output=None, image_output=None, thinking=None, redacted_thinking=None, provider_data=None, citations=None, reasoning_content=None, tool_name=None, tool_args=None, tool_call_error=None, stop_after_tool_call=False, add_to_agent_memory=True, from_history=False, metrics=MessageMetrics(input_tokens=5036, output_tokens=447, total_tokens=5483, audio_tokens=0, input_audio_tokens=0, output_audio_tokens=0, cached_tokens=0, reasoning_tokens=0, prompt_tokens=0, completion_tokens=0, prompt_tokens_details=None, completion_tokens_details=None, additional_metrics=None, time=5.5558860000019195, time_to_first_token=None, timer=<agno.utils.timer.Timer object at 0x312488410>), references=None, created_at=1746625028)], metrics={'input_tokens': [5036], 'output_tokens': [447], 'total_tokens': [5483], 'audio_tokens': [0], 'input_audio_tokens': [0], 'output_audio_tokens': [0], 'cached_tokens': [0], 'reasoning_tokens': [0], 'prompt_tokens': [0], 'completion_tokens': [0], 'time': [5.5558860000019195]}, model='models/gemini-2.0-flash-lite', run_id='83b14098-989c-4123-b3d5-cfdd1c4a24fd', agent_id='c0bbba6c-4302-42e5-bf9f-37fad7ba0b15', session_id='7ca372a4-bf75-4707-be77-19fac1a16c7b', workflow_id=None, tools=[], formatted_tool_calls=None, images=None, videos=None, audio=None, response_audio=None, citations=None, extra_data=None, created_at=1746622852)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n---- Gemini Summary ----\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593f89a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
